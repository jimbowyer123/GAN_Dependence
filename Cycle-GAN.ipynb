{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Program Files\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Program Files\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Program Files\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow_internal')\n  File \"C:\\Program Files\\Anaconda2\\lib\\importlib\\__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: No module named _pywrap_tensorflow_internal\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7be1c3b8b3b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchi2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'config IPCompleter.greedy=True'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\Lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 74\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Program Files\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Program Files\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Program Files\\Anaconda3\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\n    return importlib.import_module('_pywrap_tensorflow_internal')\n  File \"C:\\Program Files\\Anaconda2\\lib\\importlib\\__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: No module named _pywrap_tensorflow_internal\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "%config IPCompleter.greedy=True\n",
    "import import_ipynb\n",
    "from HSIC import Gram_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.path.append('C:\\Program Files\\Anaconda3\\Lib\\site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to define the hyperparameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient weighting the cycle consistency loss\n",
    "cycle_coef=2\n",
    "disc_lr=0.01\n",
    "gen_lr=0.0001\n",
    "\n",
    "# Dimensions of our 2 distributions\n",
    "dim=5\n",
    "g_hidden=[256]\n",
    "d_hidden=[256]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100000\n",
    "batch_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two generators for our models with number of hidden layers defined by the g_hidden variable.\n",
    "\n",
    "def gen_y(x,reuse=False):\n",
    "    with tf.variable_scope('gen_y',reuse=reuse):\n",
    "        w_init=tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        flow={'relu0':x}\n",
    "        \n",
    "        for i in range(len(g_hidden)):\n",
    "            \n",
    "            # Hidden layers consist of a dense layer followed by a ReLu activation\n",
    "            flow['dense{}'.format(i+1)]=tf.layers.dense(flow['relu{}'.format(i)],g_hidden[i],kernel_initializer=w_init)\n",
    "            flow['relu{}'.format(i+1)]=tf.nn.relu(flow['dense{}'.format(i+1)])\n",
    "        \n",
    "        flow['dense{}'.format(len(g_hidden)+1)] = tf.layers.dense(flow['relu{}'.format(len(g_hidden))],dim,kernel_initializer=w_init)\n",
    "        #flow['out']=tf.scalar_mul(tf.constant(2,tf.float32),tf.nn.tanh(flow['dense{}'.format(len(g_hidden)+1)]))\n",
    "        print(flow.keys())\n",
    "        return flow['dense{}'.format(len(g_hidden)+1)]\n",
    "    \n",
    "def gen_x(y,reuse=False):\n",
    "    with tf.variable_scope('gen_x',reuse=reuse):\n",
    "        w_init=tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        flow={'relu0':y}\n",
    "        \n",
    "        for i in range(len(g_hidden)):\n",
    "            flow['dense{}'.format(i+1)]=tf.layers.dense(flow['relu{}'.format(i)],g_hidden[i],kernel_initializer=w_init)\n",
    "            flow['relu{}'.format(i+1)]=tf.nn.relu(flow['dense{}'.format(i+1)])\n",
    "        \n",
    "        flow['dense{}'.format(len(g_hidden)+1)] = tf.layers.dense(flow['relu{}'.format(len(g_hidden))],dim,kernel_initializer=w_init)\n",
    "        #print(flow['dense{}'.format(len(g_hidden)+1)])\n",
    "        #flow['out']=tf.nn.tanh(flow['dense{}'.format(len(g_hidden)+1)])\n",
    "        \n",
    "        return flow['dense{}'.format(len(g_hidden)+1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two discriminators we use in our model with hidden layers defined by the d_hidden variable\n",
    "\n",
    "def disc_y(y,reuse=False):\n",
    "    with tf.variable_scope('disc_y',reuse=reuse):\n",
    "        w_init=tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        flow={'lrelu0':y}\n",
    "        \n",
    "        for i in range(len(d_hidden)):\n",
    "            # Here the hidden layers consist of dense layers followed by a leaky relu activation\n",
    "            flow['dense{}'.format(i+1)]=tf.layers.dense(flow['lrelu{}'.format(i)],d_hidden[i],kernel_initializer=w_init)\n",
    "            flow['lrelu{}'.format(i+1)]=tf.nn.leaky_relu(flow['dense{}'.format(i+1)])\n",
    "            \n",
    "        flow['dense{}'.format(len(d_hidden)+1)]=tf.layers.dense(flow['lrelu{}'.format(len(d_hidden))],1,kernel_initializer=w_init)\n",
    "        flow['out']=tf.nn.sigmoid(flow['dense{}'.format(len(d_hidden)+1)])\n",
    "        \n",
    "        return flow['out']\n",
    "    \n",
    "def disc_x(x,reuse=False):\n",
    "    with tf.variable_scope('disc_x',reuse=reuse):\n",
    "        w_init=tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        flow={'lrelu0':x}\n",
    "        \n",
    "        for i in range(len(d_hidden)):\n",
    "            flow['dense{}'.format(i+1)]=tf.layers.dense(flow['lrelu{}'.format(i)],d_hidden[i],kernel_initializer=w_init)\n",
    "            flow['lrelu{}'.format(i+1)]=tf.nn.leaky_relu(flow['dense{}'.format(i+1)])\n",
    "            \n",
    "        flow['dense{}'.format(len(d_hidden)+1)]=tf.layers.dense(flow['lrelu{}'.format(len(d_hidden))],1,kernel_initializer=w_init)\n",
    "        flow['out']=tf.nn.sigmoid(flow['dense{}'.format(len(d_hidden)+1)])\n",
    "        \n",
    "        return flow['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,shape=(None,dim))\n",
    "Y=tf.placeholder(tf.float32,shape=(None,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gen=gen_x(Y)\n",
    "Y_gen=gen_y(X)\n",
    "\n",
    "print(X_gen)\n",
    "print(Y_gen)\n",
    "\n",
    "X_recon=gen_x(Y_gen,reuse=True)\n",
    "print(X_recon)\n",
    "\n",
    "Y_recon=gen_y(X_gen,reuse=True)\n",
    "\n",
    "Disc_Y_true=disc_y(Y)\n",
    "Disc_Y_fake=disc_y(Y_gen,reuse=True)\n",
    "\n",
    "Disc_X_true=disc_x(X)\n",
    "Disc_X_fake=disc_x(X_gen,reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_X_loss=-tf.reduce_mean(tf.log(Disc_X_true)+tf.log(1-Disc_X_fake))\n",
    "D_Y_loss=-tf.reduce_mean(tf.log(Disc_Y_true)+tf.log(1-Disc_Y_fake))\n",
    "\n",
    "G_X_loss=-tf.reduce_mean(tf.log(Disc_X_fake))\n",
    "G_Y_loss=-tf.reduce_mean(tf.log(Disc_Y_fake))\n",
    "\n",
    "X_Cyc_loss=tf.losses.absolute_difference(X,X_recon)\n",
    "Y_Cyc_loss=tf.losses.absolute_difference(Y,Y_recon)\n",
    "\n",
    "Cyc_loss=X_Cyc_loss+Y_Cyc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_loss=D_X_loss+D_Y_loss\n",
    "G_loss=G_X_loss+G_Y_loss+cycle_coef*Cyc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_vars=tf.trainable_variables()\n",
    "\n",
    "D_X_vars=[var for var in T_vars if var.name.startswith('disc_x')]\n",
    "D_Y_vars=[var for var in T_vars if var.name.startswith('disc_y')]\n",
    "\n",
    "G_X_vars=[var for var in T_vars if var.name.startswith('gen_x')]\n",
    "G_Y_vars=[var for var in T_vars if var.name.startswith('gen_y')]\n",
    "\n",
    "D_vars=D_X_vars+D_Y_vars\n",
    "G_vars=G_X_vars+G_Y_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_Optimizer=tf.train.GradientDescentOptimizer(disc_lr).minimize(D_loss,var_list=D_vars)\n",
    "G_Optimizer=tf.train.AdamOptimizer(learning_rate=gen_lr).minimize(G_loss,var_list=G_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #X_=np.load('C:/Users/Jimbowyer123/Documents/GitHub/GAN_Dependence/Data/X_data.npy')\n",
    "    #Y_=np.load('C:/Users/Jimbowyer123/Documents/GitHub/GAN_Dependence/Data/Y_dependent.npy')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        X_= np.random.normal(0,1,size=(100000,100))\n",
    "        Y_= 2*X_\n",
    "        Dxl=0\n",
    "        Dyl=0\n",
    "        Gxl=0\n",
    "        Gyl=0\n",
    "        Cl=0\n",
    "        Dl=0\n",
    "        Gl=0\n",
    "        for batch in range(int(X_.shape[0]/(batch_size))):\n",
    "            X_batch1=X_[batch*batch_size:batch*batch_size+int(batch_size/2),:]\n",
    "            Y_batch1=Y_[batch*batch_size:batch*batch_size+int(batch_size/2),:]\n",
    "            X_batch2=X_[batch*batch_size+int(batch_size/2):(batch+1)*batch_size,:]\n",
    "            Y_batch2=Y_[batch*batch_size+int(batch_size/2):(batch+1)*batch_size,:]\n",
    "        \n",
    "        \n",
    "            \n",
    "            #print(X_batch1.shape)\n",
    "            #print(Y_batch1.shape)\n",
    "            \n",
    "            \n",
    "            _,dxl,dyl,gxl,gyl,cl,dl,gl=sess.run([D_Optimizer,D_X_loss,D_Y_loss,G_X_loss,G_Y_loss,Cyc_loss,D_loss,G_loss],feed_dict={X:X_batch1,Y:Y_batch1})\n",
    "            Dxl+=dxl\n",
    "            Dyl+=dyl\n",
    "            Gxl+=gxl\n",
    "            Gyl+=gyl\n",
    "            Cl+=cl\n",
    "            Dl+=dl\n",
    "            Gl+=gl\n",
    "            \n",
    "            _ = sess.run(G_Optimizer,feed_dict={X:X_batch2,Y:Y_batch2})\n",
    "            \n",
    "        if epoch%10==0:\n",
    "            X_test=np.random.normal(0,1,size=(1000,100))\n",
    "            Y_test=np.random.normal(0,2,size=(1000,100))\n",
    "            \n",
    "            X_test_gen,Y_test_gen=sess.run([X_gen,Y_gen],feed_dict={Y:Y_test,X:X_test})\n",
    "                                            \n",
    "            Y_test_gen_normalized=Y_test_gen/2\n",
    "            \n",
    "            \n",
    "            distances_x=np.zeros((1000,))\n",
    "            distances_y=np.zeros((1000,))\n",
    "            for i in range(1000):\n",
    "                point_x=X_test_gen[i,:]\n",
    "                point_y=Y_test_gen_normalized[i,:]\n",
    "                #print(point)\n",
    "                distances_x[i]=np.dot(point_x,point_x)\n",
    "                distances_y[i]=np.dot(point_y,point_y)\n",
    "            #print(distances)\n",
    "            #m_distances=np.sqrt(distances)\n",
    "            #print(m_distances)\n",
    "            ordered_distances_x=np.sort(distances_x)\n",
    "            ordered_distances_y=np.sort(distances_y)\n",
    "\n",
    "            quantiles=np.linspace(1,1000,num=1000)\n",
    "            quantiles=(quantiles-0.5)/1000\n",
    "            chi=chi2\n",
    "            quants=chi.ppf(quantiles,df=5)\n",
    "            plt.plot(quants,ordered_distances_x)\n",
    "            plt.title('Generate_X')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(quants,ordered_distances_y)\n",
    "            plt.title('Generate_Y')\n",
    "            plt.show()\n",
    "        \n",
    "        print('\\nEpoch: {}'.format(epoch))\n",
    "        print('Discriminator_X_Loss: {}'.format(Dxl))\n",
    "        print('Discriminator_Y_Loss: {}'.format(Dyl))\n",
    "        print('Generator_X_Loss: {}'.format(Gxl))\n",
    "        print('Generator_Y_Loss: {}'.format(Gyl))\n",
    "        print('Cycle_Loss: {}'.format(Cl))\n",
    "        print('Discriminator_Loss: {}'.format(Dl))\n",
    "        print('Generator_Loss: {}'.format(Gl))\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
