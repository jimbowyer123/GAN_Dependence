{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from HSIC.ipynb\n",
      "[[ 0.52247854 -0.72305133  0.42362977 ... -0.66569283  0.59173854\n",
      "  -1.05299485]\n",
      " [ 0.06243565 -0.99729137  0.34769109 ... -1.90700034 -0.13039074\n",
      "   0.12940919]\n",
      " [ 0.76667532 -0.11898342  0.10335112 ...  1.82174277  1.87134055\n",
      "  -0.19638002]\n",
      " ...\n",
      " [ 1.16650971  0.24626519 -0.37999951 ...  0.0333886  -1.99895713\n",
      "   0.9042014 ]\n",
      " [ 1.01823035  1.87367693 -1.33510842 ... -0.76256158  1.99908901\n",
      "  -1.64211916]\n",
      " [ 0.50035856 -0.64704522 -1.28932824 ...  1.35448924 -0.58889092\n",
      "   0.69516464]]\n",
      "[[-1.35643411 -0.84354406  0.59971208 ... -0.04700621  1.99607914\n",
      "  -0.18602048]\n",
      " [-0.5671831  -0.2662029  -1.45884678 ...  2.11142715 -0.07055709\n",
      "  -1.61964744]\n",
      " [-0.17661444  0.15491846 -0.71738351 ... -0.03860649  1.42079179\n",
      "   1.08891713]\n",
      " ...\n",
      " [ 0.5836179  -0.85405022  2.0207807  ...  0.61060831 -0.30785175\n",
      "   0.85222053]\n",
      " [ 0.38277667  0.68301822 -1.09029482 ... -0.40405128  1.62156537\n",
      "   0.99483959]\n",
      " [-0.96877631  0.25447938 -0.44159347 ... -0.33383655 -0.84867432\n",
      "  -0.49758993]]\n",
      "[[ 1.04495708 -1.44610266  0.84725955 ... -1.33138566  1.18347708\n",
      "  -2.1059897 ]\n",
      " [ 0.1248713  -1.99458274  0.69538217 ... -3.81400069 -0.26078149\n",
      "   0.25881838]\n",
      " [ 1.53335064 -0.23796684  0.20670225 ...  3.64348553  3.7426811\n",
      "  -0.39276004]\n",
      " ...\n",
      " [ 2.33301943  0.49253037 -0.75999902 ...  0.0667772  -3.99791426\n",
      "   1.8084028 ]\n",
      " [ 2.03646069  3.74735387 -2.67021684 ... -1.52512315  3.99817801\n",
      "  -3.28423832]\n",
      " [ 1.00071712 -1.29409043 -2.57865647 ...  2.70897847 -1.17778184\n",
      "   1.39032928]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a6f781c27cf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IPCompleter.greedy=True'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimport_ipynb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mHSIC\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGram_Matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_backward_compatible\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\import_ipynb.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(self, fullname)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_transformer_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;31m# run the code in themodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_ns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_user_ns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\GAN_Dependence\\HSIC.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\GAN_Dependence\\HSIC.ipynb\u001b[0m in \u001b[0;36mHSIC\u001b[1;34m(X, Y)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "%config IPCompleter.greedy=True\n",
    "import import_ipynb\n",
    "from HSIC import Gram_Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to define the hyperparameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient weighting the cycle consistency loss\n",
    "cycle_coef=2\n",
    "disc_lr=0.01\n",
    "gen_lr=0.0001\n",
    "\n",
    "# Dimensions of our 2 distributions\n",
    "dim=5\n",
    "g_hidden=[256]\n",
    "d_hidden=[256]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100000\n",
    "batch_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two generators for our models with number of hidden layers defined by the g_hidden variable.\n",
    "\n",
    "def gen_y(x,reuse=False):\n",
    "    with tf.variable_scope('gen_y',reuse=reuse):\n",
    "        w_init=tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        flow={'relu0':x}\n",
    "        \n",
    "        for i in range(len(g_hidden)):\n",
    "            \n",
    "            # Hidden layers consist of a dense layer followed by a ReLu activation\n",
    "            flow['dense{}'.format(i+1)]=tf.layers.dense(flow['relu{}'.format(i)],g_hidden[i],kernel_initializer=w_init)\n",
    "            flow['relu{}'.format(i+1)]=tf.nn.relu(flow['dense{}'.format(i+1)])\n",
    "        \n",
    "        flow['dense{}'.format(len(g_hidden)+1)] = tf.layers.dense(flow['relu{}'.format(len(g_hidden))],dim,kernel_initializer=w_init)\n",
    "        #flow['out']=tf.scalar_mul(tf.constant(2,tf.float32),tf.nn.tanh(flow['dense{}'.format(len(g_hidden)+1)]))\n",
    "        print(flow.keys())\n",
    "        return flow['dense{}'.format(len(g_hidden)+1)]\n",
    "    \n",
    "def gen_x(y,reuse=False):\n",
    "    with tf.variable_scope('gen_x',reuse=reuse):\n",
    "        w_init=tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        flow={'relu0':y}\n",
    "        \n",
    "        for i in range(len(g_hidden)):\n",
    "            flow['dense{}'.format(i+1)]=tf.layers.dense(flow['relu{}'.format(i)],g_hidden[i],kernel_initializer=w_init)\n",
    "            flow['relu{}'.format(i+1)]=tf.nn.relu(flow['dense{}'.format(i+1)])\n",
    "        \n",
    "        flow['dense{}'.format(len(g_hidden)+1)] = tf.layers.dense(flow['relu{}'.format(len(g_hidden))],dim,kernel_initializer=w_init)\n",
    "        #print(flow['dense{}'.format(len(g_hidden)+1)])\n",
    "        #flow['out']=tf.nn.tanh(flow['dense{}'.format(len(g_hidden)+1)])\n",
    "        \n",
    "        return flow['dense{}'.format(len(g_hidden)+1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two discriminators we use in our model with hidden layers defined by the d_hidden variable\n",
    "\n",
    "def disc_y(y,reuse=False):\n",
    "    with tf.variable_scope('disc_y',reuse=reuse):\n",
    "        w_init=tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        flow={'lrelu0':y}\n",
    "        \n",
    "        for i in range(len(d_hidden)):\n",
    "            # Here the hidden layers consist of dense layers followed by a leaky relu activation\n",
    "            flow['dense{}'.format(i+1)]=tf.layers.dense(flow['lrelu{}'.format(i)],d_hidden[i],kernel_initializer=w_init)\n",
    "            flow['lrelu{}'.format(i+1)]=tf.nn.leaky_relu(flow['dense{}'.format(i+1)])\n",
    "            \n",
    "        flow['dense{}'.format(len(d_hidden)+1)]=tf.layers.dense(flow['lrelu{}'.format(len(d_hidden))],1,kernel_initializer=w_init)\n",
    "        flow['out']=tf.nn.sigmoid(flow['dense{}'.format(len(d_hidden)+1)])\n",
    "        \n",
    "        return flow['out']\n",
    "    \n",
    "def disc_x(x,reuse=False):\n",
    "    with tf.variable_scope('disc_x',reuse=reuse):\n",
    "        w_init=tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        flow={'lrelu0':x}\n",
    "        \n",
    "        for i in range(len(d_hidden)):\n",
    "            flow['dense{}'.format(i+1)]=tf.layers.dense(flow['lrelu{}'.format(i)],d_hidden[i],kernel_initializer=w_init)\n",
    "            flow['lrelu{}'.format(i+1)]=tf.nn.leaky_relu(flow['dense{}'.format(i+1)])\n",
    "            \n",
    "        flow['dense{}'.format(len(d_hidden)+1)]=tf.layers.dense(flow['lrelu{}'.format(len(d_hidden))],1,kernel_initializer=w_init)\n",
    "        flow['out']=tf.nn.sigmoid(flow['dense{}'.format(len(d_hidden)+1)])\n",
    "        \n",
    "        return flow['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,shape=(None,dim))\n",
    "Y=tf.placeholder(tf.float32,shape=(None,dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gen=gen_x(Y)\n",
    "Y_gen=gen_y(X)\n",
    "\n",
    "print(X_gen)\n",
    "print(Y_gen)\n",
    "\n",
    "X_recon=gen_x(Y_gen,reuse=True)\n",
    "print(X_recon)\n",
    "\n",
    "Y_recon=gen_y(X_gen,reuse=True)\n",
    "\n",
    "Disc_Y_true=disc_y(Y)\n",
    "Disc_Y_fake=disc_y(Y_gen,reuse=True)\n",
    "\n",
    "Disc_X_true=disc_x(X)\n",
    "Disc_X_fake=disc_x(X_gen,reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_X_loss=-tf.reduce_mean(tf.log(Disc_X_true)+tf.log(1-Disc_X_fake))\n",
    "D_Y_loss=-tf.reduce_mean(tf.log(Disc_Y_true)+tf.log(1-Disc_Y_fake))\n",
    "\n",
    "G_X_loss=-tf.reduce_mean(tf.log(Disc_X_fake))\n",
    "G_Y_loss=-tf.reduce_mean(tf.log(Disc_Y_fake))\n",
    "\n",
    "X_Cyc_loss=tf.losses.absolute_difference(X,X_recon)\n",
    "Y_Cyc_loss=tf.losses.absolute_difference(Y,Y_recon)\n",
    "\n",
    "Cyc_loss=X_Cyc_loss+Y_Cyc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_loss=D_X_loss+D_Y_loss\n",
    "G_loss=G_X_loss+G_Y_loss+cycle_coef*Cyc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_vars=tf.trainable_variables()\n",
    "\n",
    "D_X_vars=[var for var in T_vars if var.name.startswith('disc_x')]\n",
    "D_Y_vars=[var for var in T_vars if var.name.startswith('disc_y')]\n",
    "\n",
    "G_X_vars=[var for var in T_vars if var.name.startswith('gen_x')]\n",
    "G_Y_vars=[var for var in T_vars if var.name.startswith('gen_y')]\n",
    "\n",
    "D_vars=D_X_vars+D_Y_vars\n",
    "G_vars=G_X_vars+G_Y_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_Optimizer=tf.train.GradientDescentOptimizer(disc_lr).minimize(D_loss,var_list=D_vars)\n",
    "G_Optimizer=tf.train.AdamOptimizer(learning_rate=gen_lr).minimize(G_loss,var_list=G_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #X_=np.load('C:/Users/Jimbowyer123/Documents/GitHub/GAN_Dependence/Data/X_data.npy')\n",
    "    #Y_=np.load('C:/Users/Jimbowyer123/Documents/GitHub/GAN_Dependence/Data/Y_dependent.npy')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        X_= np.random.normal(0,1,size=(100000,100))\n",
    "        Y_= 2*X_\n",
    "        Dxl=0\n",
    "        Dyl=0\n",
    "        Gxl=0\n",
    "        Gyl=0\n",
    "        Cl=0\n",
    "        Dl=0\n",
    "        Gl=0\n",
    "        for batch in range(int(X_.shape[0]/(batch_size))):\n",
    "            X_batch1=X_[batch*batch_size:batch*batch_size+int(batch_size/2),:]\n",
    "            Y_batch1=Y_[batch*batch_size:batch*batch_size+int(batch_size/2),:]\n",
    "            X_batch2=X_[batch*batch_size+int(batch_size/2):(batch+1)*batch_size,:]\n",
    "            Y_batch2=Y_[batch*batch_size+int(batch_size/2):(batch+1)*batch_size,:]\n",
    "        \n",
    "        \n",
    "            \n",
    "            #print(X_batch1.shape)\n",
    "            #print(Y_batch1.shape)\n",
    "            \n",
    "            \n",
    "            _,dxl,dyl,gxl,gyl,cl,dl,gl=sess.run([D_Optimizer,D_X_loss,D_Y_loss,G_X_loss,G_Y_loss,Cyc_loss,D_loss,G_loss],feed_dict={X:X_batch1,Y:Y_batch1})\n",
    "            Dxl+=dxl\n",
    "            Dyl+=dyl\n",
    "            Gxl+=gxl\n",
    "            Gyl+=gyl\n",
    "            Cl+=cl\n",
    "            Dl+=dl\n",
    "            Gl+=gl\n",
    "            \n",
    "            _ = sess.run(G_Optimizer,feed_dict={X:X_batch2,Y:Y_batch2})\n",
    "            \n",
    "        if epoch%10==0:\n",
    "            X_test=np.random.normal(0,1,size=(1000,100))\n",
    "            Y_test=np.random.normal(0,2,size=(1000,100))\n",
    "            \n",
    "            X_test_gen,Y_test_gen=sess.run([X_gen,Y_gen],feed_dict={Y:Y_test,X:X_test})\n",
    "                                            \n",
    "            Y_test_gen_normalized=Y_test_gen/2\n",
    "            \n",
    "            \n",
    "            distances_x=np.zeros((1000,))\n",
    "            distances_y=np.zeros((1000,))\n",
    "            for i in range(1000):\n",
    "                point_x=X_test_gen[i,:]\n",
    "                point_y=Y_test_gen_normalized[i,:]\n",
    "                #print(point)\n",
    "                distances_x[i]=np.dot(point_x,point_x)\n",
    "                distances_y[i]=np.dot(point_y,point_y)\n",
    "            #print(distances)\n",
    "            #m_distances=np.sqrt(distances)\n",
    "            #print(m_distances)\n",
    "            ordered_distances_x=np.sort(distances_x)\n",
    "            ordered_distances_y=np.sort(distances_y)\n",
    "\n",
    "            quantiles=np.linspace(1,1000,num=1000)\n",
    "            quantiles=(quantiles-0.5)/1000\n",
    "            chi=chi2\n",
    "            quants=chi.ppf(quantiles,df=5)\n",
    "            plt.plot(quants,ordered_distances_x)\n",
    "            plt.title('Generate_X')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(quants,ordered_distances_y)\n",
    "            plt.title('Generate_Y')\n",
    "            plt.show()\n",
    "        \n",
    "        print('\\nEpoch: {}'.format(epoch))\n",
    "        print('Discriminator_X_Loss: {}'.format(Dxl))\n",
    "        print('Discriminator_Y_Loss: {}'.format(Dyl))\n",
    "        print('Generator_X_Loss: {}'.format(Gxl))\n",
    "        print('Generator_Y_Loss: {}'.format(Gyl))\n",
    "        print('Cycle_Loss: {}'.format(Cl))\n",
    "        print('Discriminator_Loss: {}'.format(Dl))\n",
    "        print('Generator_Loss: {}'.format(Gl))\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
